{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My-Fashion-MNIST-Experiments\n",
    "\n",
    "In this notebook, I will design my own CNN architecture for classifying the Fashion-MNIST dataset, and will write functions such that I can experiment with different hyperparameters and loss functions/optimizers. LESHGOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that just for loading the data in, I will use pre-existing code from [this tutorial](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html), which I also used in my `fashion-mnist.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "test_data = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment - play with the number of batches I want\n",
    "# default is 4\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for our datasets; shuffle for training, not for test\n",
    "training_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Testing set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(train_data)))\n",
    "print('Testing set has {} instances'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dress  Dress  Dress  T-shirt/top  Sneaker\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample, train_label = train_data[0]\n",
    "train_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Architecture\n",
    "\n",
    "Let's go for 3 convolutions, 4 fully-connected layers, options for activation (ReLU, sigmoid, tanh), and options for pooling (max, avg). This also gives me a great chance to 1. test out different model architectures, and 2. to test out the limits of my machine (MacBook Pro M3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_dims(input_matrix_size, kernel_size):\n",
    "    output_size_n = input_matrix_size - kernel_size + 1\n",
    "    print(f'Input matrix size: {input_matrix_size}x{input_matrix_size}')\n",
    "    print(f'Kernel size: {kernel_size}x{kernel_size}')\n",
    "    print(f'Therefore output convolved matrix size: {output_size_n}x{output_size_n}')\n",
    "    return output_size_n\n",
    "\n",
    "def pooling_dims(input_matrix_size, kernel_size, stride):\n",
    "    output_size_n = int(np.floor((input_matrix_size - kernel_size) / stride) + 1)\n",
    "    print(f'Input matrix size: {input_matrix_size}x{input_matrix_size}')\n",
    "    print(f'Kernel size: {kernel_size}x{kernel_size} + stride: {stride}')\n",
    "    print(f'Therefore output pooled matrix size: {output_size_n}x{output_size_n}')\n",
    "    return output_size_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFashionClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, my_params):\n",
    "        super(MyFashionClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=my_params['conv1_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=my_params['conv1_out_channels'],\n",
    "            out_channels=my_params['conv2_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=my_params['conv2_out_channels'],\n",
    "            out_channels=my_params['conv3_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        \n",
    "        self.pool_type = my_params['pool_type']\n",
    "        if self.pool_type == 'max':\n",
    "            self.pool = nn.MaxPool2d(kernel_size=my_params['pool_kernel_size'], stride=my_params['pool_stride'])\n",
    "        elif self.pool_type == 'avg':\n",
    "            self.pool_type = nn.AvgPool2d(kernel_size=my_params['pool_kernel_size'], stride=my_params['pool_stride'])\n",
    "\n",
    "        self.activation_type = my_params['activation_type']\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=my_params['fc1_in_features'],\n",
    "            out_features=my_params['fc1_out_features']\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=my_params['fc1_out_features'],\n",
    "            out_features=my_params['fc2_out_features']\n",
    "        )\n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=my_params['fc2_out_features'],\n",
    "            out_features=my_params['fc3_out_features']\n",
    "        )\n",
    "        self.fc4 = nn.Linear(\n",
    "            in_features=my_params['fc3_out_features'],\n",
    "            out_features=10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.pool_type == 'max':\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, 128 * 2 * 2)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating Parameter Dictionary\n",
    "\n",
    "First we start by getting an idea of how our dimensions look like between layers. We will then use this to fill our parameters dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First convolutional block\n",
      "Input matrix size: 28x28\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 27x27\n",
      "\n",
      "Input matrix size: 27x27\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 13x13\n",
      "------\n",
      "Second convolutional block\n",
      "Input matrix size: 13x13\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 12x12\n",
      "\n",
      "Input matrix size: 12x12\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 6x6\n",
      "------\n",
      "Third convolutional block\n",
      "Input matrix size: 6x6\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 5x5\n",
      "\n",
      "Input matrix size: 5x5\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 2x2\n"
     ]
    }
   ],
   "source": [
    "conv_kernel_size = 2\n",
    "pool_kernel_size = 2\n",
    "pool_stride = 2\n",
    "\n",
    "print(\"First convolutional block\")\n",
    "conv1_output = cnn_dims(28, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool1_output = pooling_dims(conv1_output, pool_kernel_size, pool_stride)\n",
    "print(\"------\")\n",
    "print(\"Second convolutional block\")\n",
    "conv2_output = cnn_dims(pool1_output, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool2_output = pooling_dims(conv2_output, pool_kernel_size, pool_stride)\n",
    "print(\"------\")\n",
    "print(\"Third convolutional block\")\n",
    "conv3_output = cnn_dims(pool2_output, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool3_output = pooling_dims(conv3_output, pool_kernel_size, pool_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    # number of output channels for each convolution\n",
    "    # good rule of thumb is to double every time\n",
    "    'conv1_out_channels':32,\n",
    "    'conv2_out_channels':64,\n",
    "    'conv3_out_channels':128,\n",
    "    # number of fully-connected layers - max 4\n",
    "    'fc1_in_features':128 * 2 * 2, # x * y * y, where x=conv3_out_channels and y=dim(conv3_out_channels)\n",
    "    'fc1_out_features':256, # perhaps we can halve every time now?\n",
    "    'fc2_out_features':128,\n",
    "    'fc3_out_features':64,\n",
    "    # kernel size - same for all convolutions and for all poolings\n",
    "    'conv_kernel_size':2,\n",
    "    'pool_kernel_size':2,\n",
    "    # stride - same for all convolutions and for all poolings\n",
    "    'conv_stride':1,\n",
    "    'pool_stride':2,\n",
    "    # desired pooling type - either of {'max', 'avg'}\n",
    "    'pool_type': 'max',\n",
    "    # desired activation - either of {'relu', 'sigmoid', 'tanh'}\n",
    "    'activation_type': 'relu',\n",
    "    # optimizer - either of {'sgd', 'adam'}\n",
    "    'optimizer':'sgd'\n",
    "}\n",
    "\n",
    "# instantiating the model\n",
    "fashion_classifier_model = MyFashionClassifier(my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyFashionClassifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_optimizer(model, optimizer_choice, learning_rate, momentum=None):\n",
    "    # function to choose optimizer\n",
    "    if optimizer_choice == 'sgd': \n",
    "        optimizer = torch.optim.SGD(\n",
    "            params=model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            momentum=momentum\n",
    "            ) \n",
    "    elif optimizer_choice=='adam': \n",
    "        optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "my_optimizer = choose_optimizer(\n",
    "    model=fashion_classifier_model, \n",
    "    optimizer_choice=my_params['optimizer'], \n",
    "    learning_rate=0.001,\n",
    "    momentum=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy_curves(losses, accuracies, epoch_num):\n",
    "    # plotting loss vs epoch\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title(f'Epoch {epoch_num}: Loss Curve')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    # plotting accuracy vs epoch\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accuracies)\n",
    "    plt.title(f'Epoch {epoch_num}: Accuracy Curve (avg={np.round(np.mean(accuracies),2)}%)')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([9, 2, 5, 9, 1])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# # Inspect one batch of data\n",
    "# for i, (inputs, labels) in enumerate(training_loader):\n",
    "#     print(f\"Batch {i+1}:\")\n",
    "#     # print(\"Inputs:\")\n",
    "#     # print(type(inputs))\n",
    "#     # print(\"Labels:\")\n",
    "#     # print(labels)\n",
    "#     # print(type(labels))\n",
    "#     outputs = fashion_classifier_model(inputs)\n",
    "#     print(type(outputs))\n",
    "#     # print(outputs)\n",
    "#     loss = loss_fn(outputs, labels)\n",
    "#     print(type(loss))\n",
    "#     loss.backward()\n",
    "#     my_optimizer.step()\n",
    "#     # print(outputs)\n",
    "#     predicted = torch.argmax(outputs, 1) #argmax\n",
    "#     print(predicted)\n",
    "#     print((predicted == labels).sum().item())\n",
    "\n",
    "#     break  # Break after inspecting the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model, params, optimizer, loss_fn, training_loader, num_epochs=10\n",
    "        ):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # iterating over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        current_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(training_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            \n",
    "\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item() # look into settubg them as long integers\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "\n",
    "                last_loss = current_loss / 100\n",
    "                losses.append(last_loss)\n",
    "\n",
    "                accuracy = 100 * total_correct / total_samples\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch {i+1}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "                current_loss = 0\n",
    "                total_correct = 0\n",
    "                total_samples = 0\n",
    "                \n",
    "        plot_loss_accuracy_curves(losses, accuracies, 1)\n",
    "\n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Batch 1, Loss: 1.5489, Accuracy: 60.0000\n",
      "Epoch [1/1], Batch 101, Loss: 0.0167, Accuracy: 93.2000\n",
      "Epoch [1/1], Batch 201, Loss: 0.0962, Accuracy: 93.2000\n",
      "Epoch [1/1], Batch 301, Loss: 1.5378, Accuracy: 90.6000\n",
      "Epoch [1/1], Batch 401, Loss: 0.1870, Accuracy: 90.4000\n",
      "Epoch [1/1], Batch 501, Loss: 0.0559, Accuracy: 91.8000\n",
      "Epoch [1/1], Batch 601, Loss: 0.0360, Accuracy: 92.4000\n",
      "Epoch [1/1], Batch 701, Loss: 0.0480, Accuracy: 91.8000\n",
      "Epoch [1/1], Batch 801, Loss: 0.0253, Accuracy: 90.2000\n",
      "Epoch [1/1], Batch 901, Loss: 0.0754, Accuracy: 92.8000\n",
      "Epoch [1/1], Batch 1001, Loss: 0.5239, Accuracy: 91.0000\n",
      "Epoch [1/1], Batch 1101, Loss: 0.1199, Accuracy: 89.6000\n",
      "Epoch [1/1], Batch 1201, Loss: 0.4211, Accuracy: 90.8000\n",
      "Epoch [1/1], Batch 1301, Loss: 0.0323, Accuracy: 91.4000\n",
      "Epoch [1/1], Batch 1401, Loss: 0.2708, Accuracy: 91.2000\n",
      "Epoch [1/1], Batch 1501, Loss: 0.5253, Accuracy: 91.2000\n",
      "Epoch [1/1], Batch 1601, Loss: 0.3808, Accuracy: 90.6000\n",
      "Epoch [1/1], Batch 1701, Loss: 0.0066, Accuracy: 91.8000\n",
      "Epoch [1/1], Batch 1801, Loss: 0.3424, Accuracy: 91.4000\n",
      "Epoch [1/1], Batch 1901, Loss: 0.0159, Accuracy: 91.8000\n",
      "Epoch [1/1], Batch 2001, Loss: 0.0312, Accuracy: 90.8000\n",
      "Epoch [1/1], Batch 2101, Loss: 0.2961, Accuracy: 91.2000\n",
      "Epoch [1/1], Batch 2201, Loss: 0.0946, Accuracy: 91.2000\n",
      "Epoch [1/1], Batch 2301, Loss: 0.1438, Accuracy: 92.2000\n",
      "Epoch [1/1], Batch 2401, Loss: 0.0152, Accuracy: 93.4000\n",
      "Epoch [1/1], Batch 2501, Loss: 0.2545, Accuracy: 92.4000\n",
      "Epoch [1/1], Batch 2601, Loss: 0.6695, Accuracy: 91.4000\n",
      "Epoch [1/1], Batch 2701, Loss: 0.0071, Accuracy: 90.4000\n",
      "Epoch [1/1], Batch 2801, Loss: 0.2858, Accuracy: 92.2000\n",
      "Epoch [1/1], Batch 2901, Loss: 0.0061, Accuracy: 90.8000\n",
      "Epoch [1/1], Batch 3001, Loss: 0.0137, Accuracy: 93.0000\n",
      "Epoch [1/1], Batch 3101, Loss: 0.0194, Accuracy: 89.6000\n",
      "Epoch [1/1], Batch 3201, Loss: 0.0020, Accuracy: 90.4000\n",
      "Epoch [1/1], Batch 3301, Loss: 0.3488, Accuracy: 94.2000\n",
      "Epoch [1/1], Batch 3401, Loss: 0.3885, Accuracy: 87.4000\n",
      "Epoch [1/1], Batch 3501, Loss: 0.0886, Accuracy: 90.8000\n",
      "Epoch [1/1], Batch 3601, Loss: 0.1193, Accuracy: 90.0000\n",
      "Epoch [1/1], Batch 3701, Loss: 0.0242, Accuracy: 92.4000\n",
      "Epoch [1/1], Batch 3801, Loss: 0.3829, Accuracy: 90.4000\n",
      "Epoch [1/1], Batch 3901, Loss: 0.1655, Accuracy: 90.2000\n",
      "Epoch [1/1], Batch 4001, Loss: 0.5975, Accuracy: 92.6000\n",
      "Epoch [1/1], Batch 4101, Loss: 0.0031, Accuracy: 90.2000\n",
      "Epoch [1/1], Batch 4201, Loss: 0.1197, Accuracy: 91.4000\n",
      "Epoch [1/1], Batch 4301, Loss: 0.0157, Accuracy: 92.0000\n",
      "Epoch [1/1], Batch 4401, Loss: 0.0226, Accuracy: 92.6000\n",
      "Epoch [1/1], Batch 4501, Loss: 0.0147, Accuracy: 92.8000\n",
      "Epoch [1/1], Batch 4601, Loss: 0.0336, Accuracy: 92.2000\n",
      "Epoch [1/1], Batch 4701, Loss: 0.4089, Accuracy: 89.0000\n",
      "Epoch [1/1], Batch 4801, Loss: 0.2394, Accuracy: 92.4000\n",
      "Epoch [1/1], Batch 4901, Loss: 0.0658, Accuracy: 89.0000\n",
      "Epoch [1/1], Batch 5001, Loss: 1.6317, Accuracy: 92.2000\n",
      "Epoch [1/1], Batch 5101, Loss: 0.2938, Accuracy: 88.0000\n",
      "Epoch [1/1], Batch 5201, Loss: 0.0006, Accuracy: 91.4000\n",
      "Epoch [1/1], Batch 5301, Loss: 0.0075, Accuracy: 91.8000\n",
      "Epoch [1/1], Batch 5401, Loss: 0.1078, Accuracy: 92.2000\n",
      "Epoch [1/1], Batch 5501, Loss: 0.0638, Accuracy: 92.4000\n",
      "Epoch [1/1], Batch 5601, Loss: 0.0318, Accuracy: 90.8000\n",
      "Epoch [1/1], Batch 5701, Loss: 0.0854, Accuracy: 91.4000\n",
      "Epoch [1/1], Batch 5801, Loss: 0.8836, Accuracy: 91.6000\n",
      "Epoch [1/1], Batch 5901, Loss: 0.1152, Accuracy: 90.2000\n",
      "Epoch [1/1], Batch 6001, Loss: 0.0081, Accuracy: 90.6000\n",
      "Epoch [1/1], Batch 6101, Loss: 0.4103, Accuracy: 89.8000\n",
      "Epoch [1/1], Batch 6201, Loss: 0.1458, Accuracy: 92.4000\n",
      "Epoch [1/1], Batch 6301, Loss: 0.1389, Accuracy: 94.2000\n",
      "Epoch [1/1], Batch 6401, Loss: 0.1437, Accuracy: 90.8000\n",
      "Epoch [1/1], Batch 6501, Loss: 0.9046, Accuracy: 91.4000\n",
      "Epoch [1/1], Batch 6601, Loss: 0.3615, Accuracy: 89.2000\n",
      "Epoch [1/1], Batch 6701, Loss: 0.2728, Accuracy: 89.6000\n",
      "Epoch [1/1], Batch 6801, Loss: 0.1174, Accuracy: 91.2000\n",
      "Epoch [1/1], Batch 6901, Loss: 1.3803, Accuracy: 91.8000\n",
      "Epoch [1/1], Batch 7001, Loss: 0.0838, Accuracy: 89.4000\n",
      "Epoch [1/1], Batch 7101, Loss: 0.1435, Accuracy: 91.2000\n",
      "Epoch [1/1], Batch 7201, Loss: 0.0496, Accuracy: 91.8000\n",
      "Epoch [1/1], Batch 7301, Loss: 0.2623, Accuracy: 90.6000\n",
      "Epoch [1/1], Batch 7401, Loss: 0.1603, Accuracy: 92.0000\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies = train_model(\n",
    "    model=fashion_classifier_model, \n",
    "    params=my_params, \n",
    "    optimizer=my_optimizer, \n",
    "    loss_fn=loss_fn, \n",
    "    training_loader=training_loader,\n",
    "    num_epochs=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(88.18333333333334)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, params, optimizer, loss_fn, train_loader, epoch_index, loss_list, accuracy_list):\n",
    "#     model.train()\n",
    "#     running_loss = 0\n",
    "#     total_correct = 0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     for i, (inputs, labels) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = fashion_classifier_model(inputs)\n",
    "#         loss = loss_fn(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         total_correct += (predicted == labels).sum().item()\n",
    "#         total_samples += labels.size(0)\n",
    "\n",
    "#         if i % 1000 == 999:\n",
    "#             last_loss = running_loss / 1000\n",
    "#             loss_list.append(last_loss)\n",
    "\n",
    "#             accuracy = 100 * total_correct / total_samples\n",
    "#             accuracy_list.append(accuracy)\n",
    "\n",
    "#             print(running_loss, len(train_loader), total_correct, total_samples)\n",
    "\n",
    "#             print(f\"Epoch {epoch_index+1}, Batch {i+1}, Loss: {last_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "#             running_loss = 0\n",
    "#             total_correct = 0\n",
    "#             total_samples = 0\n",
    "            \n",
    "#     return running_loss / len(train_loader), 100.0 * total_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.5049269270967 12000 4211 5000\n",
      "Epoch 1, Batch 1000, Loss: 0.4235, Accuracy: 84.22%\n",
      "404.5518905025092 12000 4235 5000\n",
      "Epoch 1, Batch 2000, Loss: 0.4046, Accuracy: 84.70%\n",
      "410.7615852156596 12000 4252 5000\n",
      "Epoch 1, Batch 3000, Loss: 0.4108, Accuracy: 85.04%\n",
      "417.22262729442446 12000 4215 5000\n",
      "Epoch 1, Batch 4000, Loss: 0.4172, Accuracy: 84.30%\n",
      "402.0472679180093 12000 4261 5000\n",
      "Epoch 1, Batch 5000, Loss: 0.4020, Accuracy: 85.22%\n",
      "396.30894401686965 12000 4275 5000\n",
      "Epoch 1, Batch 6000, Loss: 0.3963, Accuracy: 85.50%\n",
      "389.9798663900583 12000 4278 5000\n",
      "Epoch 1, Batch 7000, Loss: 0.3900, Accuracy: 85.56%\n",
      "385.46019611140946 12000 4266 5000\n",
      "Epoch 1, Batch 8000, Loss: 0.3855, Accuracy: 85.32%\n",
      "383.9879690274829 12000 4290 5000\n",
      "Epoch 1, Batch 9000, Loss: 0.3840, Accuracy: 85.80%\n",
      "362.3597458624863 12000 4299 5000\n",
      "Epoch 1, Batch 10000, Loss: 0.3624, Accuracy: 85.98%\n",
      "365.5963642257266 12000 4315 5000\n",
      "Epoch 1, Batch 11000, Loss: 0.3656, Accuracy: 86.30%\n",
      "377.491388071212 12000 4313 5000\n",
      "Epoch 1, Batch 12000, Loss: 0.3775, Accuracy: 86.26%\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfashion_classifier_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccuracy_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, params, optimizer, loss_fn, train_loader, epoch_index, loss_list, accuracy_list)\u001b[0m\n\u001b[1;32m     31\u001b[0m         total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m         total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader), \u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_correct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# train_epoch(\n",
    "#     model=fashion_classifier_model, \n",
    "#     params=my_params, \n",
    "#     optimizer=my_optimizer, \n",
    "#     loss_fn=loss_fn, \n",
    "#     train_loader=training_loader, \n",
    "#     epoch_index=0, \n",
    "#     loss_list=[], \n",
    "#     accuracy_list=[]\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mnist_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
