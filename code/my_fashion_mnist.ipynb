{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My-Fashion-MNIST-Experiments\n",
    "\n",
    "In this notebook, I will design my own CNN architecture for classifying the Fashion-MNIST dataset, and will write functions such that I can experiment with different hyperparameters and loss functions/optimizers. LESHGOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that just for loading the data in, I will use pre-existing code from [this tutorial](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html), which I also used in my `fashion-mnist.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "test_data = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment - play with the number of batches I want\n",
    "# default is 4\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for our datasets; shuffle for training, not for test\n",
    "training_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Testing set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(train_data)))\n",
    "print('Testing set has {} instances'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sneaker  Ankle Boot  Ankle Boot  Pullover  Sneaker\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample, train_label = train_data[0]\n",
    "train_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Architecture\n",
    "\n",
    "Let's go for 3 convolutions, 4 fully-connected layers, options for activation (ReLU, sigmoid, tanh), and options for pooling (max, avg). This also gives me a great chance to 1. test out different model architectures, and 2. to test out the limits of my machine (MacBook Pro M3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_dims(input_matrix_size, kernel_size):\n",
    "    output_size_n = input_matrix_size - kernel_size + 1\n",
    "    print(f'Input matrix size: {input_matrix_size}x{input_matrix_size}')\n",
    "    print(f'Kernel size: {kernel_size}x{kernel_size}')\n",
    "    print(f'Therefore output convolved matrix size: {output_size_n}x{output_size_n}')\n",
    "    return output_size_n\n",
    "\n",
    "def pooling_dims(input_matrix_size, kernel_size, stride):\n",
    "    output_size_n = int(np.floor((input_matrix_size - kernel_size) / stride) + 1)\n",
    "    print(f'Input matrix size: {input_matrix_size}x{input_matrix_size}')\n",
    "    print(f'Kernel size: {kernel_size}x{kernel_size} + stride: {stride}')\n",
    "    print(f'Therefore output pooled matrix size: {output_size_n}x{output_size_n}')\n",
    "    return output_size_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input matrix size: 28x28\n",
      "Kernel size: 5x5\n",
      "Therefore output convolved matrix size: 24x24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First convolutional block\n",
      "Input matrix size: 28x28\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 27x27\n",
      "\n",
      "Input matrix size: 27x27\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 13x13\n",
      "------\n",
      "Second convolutional block\n",
      "Input matrix size: 13x13\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 12x12\n",
      "\n",
      "Input matrix size: 12x12\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 6x6\n",
      "------\n",
      "Third convolutional block\n",
      "Input matrix size: 6x6\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 5x5\n",
      "\n",
      "Input matrix size: 5x5\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 2x2\n"
     ]
    }
   ],
   "source": [
    "conv_kernel_size = 2\n",
    "pool_kernel_size = 2\n",
    "pool_stride = 2\n",
    "\n",
    "print(\"First convolutional block\")\n",
    "conv1_output = cnn_dims(28, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool1_output = pooling_dims(conv1_output, pool_kernel_size, pool_stride)\n",
    "print(\"------\")\n",
    "print(\"Second convolutional block\")\n",
    "conv2_output = cnn_dims(pool1_output, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool2_output = pooling_dims(conv2_output, pool_kernel_size, pool_stride)\n",
    "print(\"------\")\n",
    "print(\"Third convolutional block\")\n",
    "conv3_output = cnn_dims(pool2_output, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool3_output = pooling_dims(conv3_output, pool_kernel_size, pool_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFashionClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,my_params):\n",
    "        super(MyFashionClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=my_params['conv1_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=my_params['conv1_out_channels'],\n",
    "            out_channels=my_params['conv2_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=my_params['conv2_out_channels'],\n",
    "            out_channels=my_params['conv3_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        \n",
    "        self.pool_type = my_params['pool_type']\n",
    "        if self.pool_type == 'max':\n",
    "            self.pool = nn.MaxPool2d(kernel_size=my_params['pool_kernel_size'], stride=my_params['pool_stride'])\n",
    "        elif self.pool_type == 'avg':\n",
    "            self.pool_type = nn.AvgPool2d(kernel_size=my_params['pool_kernel_size'], stride=my_params['pool_stride'])\n",
    "\n",
    "        self.activation_type = my_params['activation_type']\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=my_params['fc1_in_features'],\n",
    "            out_features=my_params['fc1_out_features']\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=my_params['fc1_out_features'],\n",
    "            out_features=my_params['fc2_out_features']\n",
    "        )\n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=my_params['fc2_out_features'],\n",
    "            out_features=my_params['fc3_out_features']\n",
    "        )\n",
    "        self.fc4 = nn.Linear(\n",
    "            in_features=my_params['fc3_out_features'],\n",
    "            out_features=10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.pool_type == 'max':\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, self._get_flattened_size(x))\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    # number of output channels for 1st convolution\n",
    "    'conv1_out_channels':32,\n",
    "    'conv2_out_channels':64,\n",
    "    'conv3_out_channels':128,\n",
    "    # number of fully-connected layers - max 4\n",
    "    'fc1_in_features':128 * 3 * 3,\n",
    "    'fc1_out_features':256,\n",
    "    'fc2_out_features':128,\n",
    "    'fc3_out_features':64,\n",
    "    # kernel size - same for all convolutions\n",
    "    'conv_kernel_size':5,\n",
    "    'pool_kernel_size':2,\n",
    "    # stride - same for all convolutions\n",
    "    'conv_stride':1,\n",
    "    'pool_stride':2,\n",
    "    # desired pooling type - either of {'max', 'avg'}\n",
    "    'pool_type': 'max',\n",
    "    # desired activation - either of {'relu', 'sigmoid', 'tanh'}\n",
    "    'activation_type': 'relu',\n",
    "    # optimizer - either of {'sgd', 'adam'}\n",
    "    'optimizer':'sgd'\n",
    "}\n",
    "\n",
    "# instantiating the model\n",
    "fashion_classifier_model = MyFashionClassifier(my_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_optimizer(model, optimizer_choice, learning_rate, momentum=None):\n",
    "    # function to choose optimizer\n",
    "    if optimizer_choice == 'sgd': \n",
    "        optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum) \n",
    "    elif optimizer_choice=='adam': \n",
    "        optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "my_optimizer = choose_optimizer(\n",
    "    fashion_classifier_model, \n",
    "    my_params['optimizer'], \n",
    "    learning_rate=0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model():\n",
    "#     training_losses = []\n",
    "#     training_accuracies = []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loss_fn, train_loader, epoch_index, loss_list, accuracy_list):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, data in enumerate(train_data):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fashion_classifier_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.steps()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000\n",
    "            loss_list.append(last_loss)\n",
    "\n",
    "            accuracy = 100 * total_correct / total_samples\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            print(f\"Epoch {epoch_index+1}, Batch {i+1}, Loss: {last_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "            running_loss = 0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "\n",
    "    return running_loss / len(train_loader), 100.0 * total_correct / total_samples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mnist_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
