{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My-Fashion-MNIST-Experiments\n",
    "\n",
    "In this notebook, I will design my own CNN architecture for classifying the Fashion-MNIST dataset, and will write functions such that I can experiment with different hyperparameters and loss functions/optimizers. LESHGOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that just for loading the data in, I will use pre-existing code from [this tutorial](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html), which I also used in my `fashion-mnist.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "test_data = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment - play with the number of batches I want\n",
    "# default is 4\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for our datasets; shuffle for training, not for test\n",
    "training_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Testing set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(train_data)))\n",
    "print('Testing set has {} instances'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pullover  T-shirt/top  Shirt  Bag  Dress\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample, train_label = train_data[0]\n",
    "train_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model Architecture\n",
    "\n",
    "Let's go for 3 convolutions, 4 fully-connected layers, options for activation (ReLU, sigmoid, tanh), and options for pooling (max, avg). This also gives me a great chance to 1. test out different model architectures, and 2. to test out the limits of my machine (MacBook Pro M3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_dims(input_matrix_size, kernel_size):\n",
    "    output_size_n = input_matrix_size - kernel_size + 1\n",
    "    print(f'Input matrix size: {input_matrix_size}x{input_matrix_size}')\n",
    "    print(f'Kernel size: {kernel_size}x{kernel_size}')\n",
    "    print(f'Therefore output convolved matrix size: {output_size_n}x{output_size_n}')\n",
    "    return output_size_n\n",
    "\n",
    "def pooling_dims(input_matrix_size, kernel_size, stride):\n",
    "    output_size_n = int(np.floor((input_matrix_size - kernel_size) / stride) + 1)\n",
    "    print(f'Input matrix size: {input_matrix_size}x{input_matrix_size}')\n",
    "    print(f'Kernel size: {kernel_size}x{kernel_size} + stride: {stride}')\n",
    "    print(f'Therefore output pooled matrix size: {output_size_n}x{output_size_n}')\n",
    "    return output_size_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFashionClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, my_params):\n",
    "        super(MyFashionClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=my_params['conv1_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=my_params['conv1_out_channels'],\n",
    "            out_channels=my_params['conv2_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=my_params['conv2_out_channels'],\n",
    "            out_channels=my_params['conv3_out_channels'],\n",
    "            kernel_size=my_params['conv_kernel_size'],\n",
    "            stride=my_params['conv_stride']\n",
    "        )\n",
    "        \n",
    "        self.pool_type = my_params['pool_type']\n",
    "        if self.pool_type == 'max':\n",
    "            self.pool = nn.MaxPool2d(kernel_size=my_params['pool_kernel_size'], stride=my_params['pool_stride'])\n",
    "        elif self.pool_type == 'avg':\n",
    "            self.pool_type = nn.AvgPool2d(kernel_size=my_params['pool_kernel_size'], stride=my_params['pool_stride'])\n",
    "\n",
    "        self.activation_type = my_params['activation_type']\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=my_params['fc1_in_features'],\n",
    "            out_features=my_params['fc1_out_features']\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=my_params['fc1_out_features'],\n",
    "            out_features=my_params['fc2_out_features']\n",
    "        )\n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=my_params['fc2_out_features'],\n",
    "            out_features=my_params['fc3_out_features']\n",
    "        )\n",
    "        self.fc4 = nn.Linear(\n",
    "            in_features=my_params['fc3_out_features'],\n",
    "            out_features=10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if self.pool_type == 'max':\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.view(-1, 128 * 2 * 2)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating Parameter Dictionary\n",
    "\n",
    "First we start by getting an idea of how our dimensions look like between layers. We will then use this to fill our parameters dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First convolutional block\n",
      "Input matrix size: 28x28\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 27x27\n",
      "\n",
      "Input matrix size: 27x27\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 13x13\n",
      "------\n",
      "Second convolutional block\n",
      "Input matrix size: 13x13\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 12x12\n",
      "\n",
      "Input matrix size: 12x12\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 6x6\n",
      "------\n",
      "Third convolutional block\n",
      "Input matrix size: 6x6\n",
      "Kernel size: 2x2\n",
      "Therefore output convolved matrix size: 5x5\n",
      "\n",
      "Input matrix size: 5x5\n",
      "Kernel size: 2x2 + stride: 2\n",
      "Therefore output pooled matrix size: 2x2\n"
     ]
    }
   ],
   "source": [
    "conv_kernel_size = 2\n",
    "pool_kernel_size = 2\n",
    "pool_stride = 2\n",
    "\n",
    "print(\"First convolutional block\")\n",
    "conv1_output = cnn_dims(28, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool1_output = pooling_dims(conv1_output, pool_kernel_size, pool_stride)\n",
    "print(\"------\")\n",
    "print(\"Second convolutional block\")\n",
    "conv2_output = cnn_dims(pool1_output, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool2_output = pooling_dims(conv2_output, pool_kernel_size, pool_stride)\n",
    "print(\"------\")\n",
    "print(\"Third convolutional block\")\n",
    "conv3_output = cnn_dims(pool2_output, conv_kernel_size)\n",
    "print(\"\")\n",
    "pool3_output = pooling_dims(conv3_output, pool_kernel_size, pool_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_params = {\n",
    "    # number of output channels for each convolution\n",
    "    # good rule of thumb is to double every time\n",
    "    'conv1_out_channels':32,\n",
    "    'conv2_out_channels':64,\n",
    "    'conv3_out_channels':128,\n",
    "    # number of fully-connected layers - max 4\n",
    "    'fc1_in_features':128 * 2 * 2, # x * y * y, where x=conv3_out_channels and y=dim(conv3_out_channels)\n",
    "    'fc1_out_features':256, # perhaps we can halve every time now?\n",
    "    'fc2_out_features':128,\n",
    "    'fc3_out_features':64,\n",
    "    # kernel size - same for all convolutions and for all poolings\n",
    "    'conv_kernel_size':2,\n",
    "    'pool_kernel_size':2,\n",
    "    # stride - same for all convolutions and for all poolings\n",
    "    'conv_stride':1,\n",
    "    'pool_stride':2,\n",
    "    # desired pooling type - either of {'max', 'avg'}\n",
    "    'pool_type': 'max',\n",
    "    # desired activation - either of {'relu', 'sigmoid', 'tanh'}\n",
    "    'activation_type': 'relu',\n",
    "    # optimizer - either of {'sgd', 'adam'}\n",
    "    'optimizer':'sgd'\n",
    "}\n",
    "\n",
    "# instantiating the model\n",
    "fashion_classifier_model = MyFashionClassifier(my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyFashionClassifier(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_classifier_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_optimizer(model, optimizer_choice, learning_rate, momentum=None):\n",
    "    # function to choose optimizer\n",
    "    if optimizer_choice == 'sgd': \n",
    "        optimizer = torch.optim.SGD(\n",
    "            params=model.parameters(), \n",
    "            lr=learning_rate, \n",
    "            momentum=momentum\n",
    "            ) \n",
    "    elif optimizer_choice=='adam': \n",
    "        optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "my_optimizer = choose_optimizer(\n",
    "    model=fashion_classifier_model, \n",
    "    optimizer_choice=my_params['optimizer'], \n",
    "    learning_rate=0.001,\n",
    "    momentum=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, params, optimizer, loss_fn, train_loader, epoch_index, loss_list, accuracy_list):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fashion_classifier_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000\n",
    "            loss_list.append(last_loss)\n",
    "\n",
    "            accuracy = 100 * total_correct / total_samples\n",
    "            accuracy_list.append(accuracy)\n",
    "\n",
    "            print(f\"Epoch {epoch_index+1}, Batch {i+1}, Loss: {last_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "            running_loss = 0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "\n",
    "    return running_loss / len(train_loader), 100.0 * total_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SGD' object has no attribute 'steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(loss))\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmy_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Break after inspecting the first batch\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SGD' object has no attribute 'steps'"
     ]
    }
   ],
   "source": [
    "# Inspect one batch of data\n",
    "for i, (inputs, labels) in enumerate(training_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    # print(\"Inputs:\")\n",
    "    # print(type(inputs))\n",
    "    # print(\"Labels:\")\n",
    "    # print(labels)\n",
    "    # print(type(labels))\n",
    "    outputs = fashion_classifier_model(inputs)\n",
    "    print(type(outputs))\n",
    "    # print(outputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    print(type(loss))\n",
    "    loss.backward()\n",
    "    my_optimizer.steps()\n",
    "\n",
    "    break  # Break after inspecting the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 1000, Loss: 2.3004, Accuracy: 12.30%\n",
      "Epoch 2, Batch 2000, Loss: 2.2229, Accuracy: 23.96%\n",
      "Epoch 2, Batch 3000, Loss: 1.3599, Accuracy: 42.94%\n",
      "Epoch 2, Batch 4000, Loss: 0.8872, Accuracy: 65.68%\n",
      "Epoch 2, Batch 5000, Loss: 0.8281, Accuracy: 69.34%\n",
      "Epoch 2, Batch 6000, Loss: 0.7044, Accuracy: 74.20%\n",
      "Epoch 2, Batch 7000, Loss: 0.6695, Accuracy: 75.32%\n",
      "Epoch 2, Batch 8000, Loss: 0.6364, Accuracy: 76.70%\n",
      "Epoch 2, Batch 9000, Loss: 0.5761, Accuracy: 78.78%\n",
      "Epoch 2, Batch 10000, Loss: 0.5596, Accuracy: 78.96%\n",
      "Epoch 2, Batch 11000, Loss: 0.5273, Accuracy: 80.50%\n",
      "Epoch 2, Batch 12000, Loss: 0.5382, Accuracy: 79.76%\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfashion_classifier_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccuracy_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 33\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, params, optimizer, loss_fn, train_loader, epoch_index, loss_list, accuracy_list)\u001b[0m\n\u001b[1;32m     30\u001b[0m         total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m         total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader), \u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_correct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "train_epoch(\n",
    "    model=fashion_classifier_model, \n",
    "    params=my_params, \n",
    "    optimizer=my_optimizer, \n",
    "    loss_fn=loss_fn, \n",
    "    train_loader=training_loader, \n",
    "    epoch_index=1, \n",
    "    loss_list=[], \n",
    "    accuracy_list=[]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mnist_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
